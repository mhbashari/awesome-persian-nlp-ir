# Models

- [Models](#models)
  - [Named Entity Recognition](#named-entity-recognition)
  - [Text Classification](#text-classification)
  - [Sentiment Analysis](#sentiment-analysis)
  - [Summarization](#summarization)
  - [Question Answering](#question-answering)
  - [Embeddings](#embeddings)
  - [Language Model](#language-model)

## Named Entity Recognition
- [ParsBERT-NER](https://github.com/hooshvare/parsbert-ner) - It is a fine-tuned model based on ParsBERT (a monolingual Persian language model) on a vast range of dataset PEYMA, ARMAN, and PEYMA+ARMAN.
- [ALBERT-NER](https://github.com/m3hrdadfi/albert-persian) - It is a fine-tuned on PEYMA and ARMAN dataset based on ALBERT Language Model.

## Text Classification
- [ParsBERT DigiMag](https://github.com/hooshvare/parsbert)
- [ParsBERT Persian News](https://github.com/hooshvare/parsbert)
- [ALBERT DigiMag](https://github.com/m3hrdadfi/albert-persian)
- [ALBERT Persian News](https://github.com/m3hrdadfi/albert-persian)

## Sentiment Analysis
- [DeepSentiPers](https://github.com/JoyeBright/DeepSentiPers)
- [ParsBERT Digikala OpenData](https://github.com/hooshvare/parsbert)
- [ParsBERT SnappFood](https://github.com/hooshvare/parsbert)
- [ParsBERT DeepSentiPers Multi](https://github.com/hooshvare/parsbert)
- [ParsBERT DeepSentiPers Binary](https://github.com/hooshvare/parsbert)
- [ALBERT Digikala OpenData](https://github.com/m3hrdadfi/albert-persian)
- [ALBERT SnappFood](https://github.com/m3hrdadfi/albert-persian)
- [ALBERT DeepSentiPers Multi](https://github.com/m3hrdadfi/albert-persian)
- [ALBERT DeepSentiPers Binary](https://github.com/m3hrdadfi/albert-persian)
- [mT5 trained on ParsiNLU-ABSA](https://huggingface.co/persiannlp/mt5-base-parsinlu-sentiment-analysis?text=%DB%8C%DA%A9+%D9%81%DB%8C%D9%84%D9%85+%D8%B6%D8%B9%DB%8C%D9%81+%D8%A8%DB%8C+%D9%85%D8%AD%D8%AA%D9%88%D8%A7+%D8%A8%D8%AF%D9%88%D9%86+%D9%81%DB%8C%D9%84%D9%85%D9%86%D8%A7%D9%85%D9%87+.+%D8%B4%D9%88%D8%AE%DB%8C+%D9%87%D8%A7%DB%8C+%D8%B3%D8%AE%DB%8C%D9%81+.+%3Csep%3E+%D9%86%D8%B8%D8%B1+%D8%B4%D9%85%D8%A7+%D8%AF%D8%B1+%D9%85%D9%88%D8%B1%D8%AF+%D8%AF%D8%A7%D8%B3%D8%AA%D8%A7%D9%86%D8%8C+%D9%81%DB%8C%D9%84%D9%85%D9%86%D8%A7%D9%85%D9%87%D8%8C+%D8%AF%DB%8C%D8%A7%D9%84%D9%88%DA%AF+%D9%87%D8%A7+%D9%88+%D9%85%D9%88%D8%B6%D9%88%D8%B9+%D9%81%DB%8C%D9%84%D9%85++%D9%84%D9%88%D9%86%D9%87+%D8%B2%D9%86%D8%A8%D9%88%D8%B1+%DA%86%DB%8C%D8%B3%D8%AA%D8%9F+)


## Summarization
- [BERT2BERT](https://github.com/m3hrdadfi/wiki-summary) - BERT2BERT is the first pre-trained summarization model trained on Wiki Summary based on ParsBERT. 

## Question Answering
- [bert-base-fa-qa on PersianQA](https://huggingface.co/SajjadAyoubi/bert-base-fa-qa)
- [xlm-roberta-large-fa-qa on PersianQA](https://huggingface.co/SajjadAyoubi/xlm-roberta-large-fa-qa)

## Multiple-Choice QA
- [mT5 trained on ParsiNLU-MCQA](https://huggingface.co/persiannlp/mt5-base-parsinlu-arc-comqa-obqa-multiple-choice?text=%D9%BE%D8%A7%DB%8C%D8%AA%D8%AE%D8%AA+%DA%A9%D8%B4%D9%88%D8%B1+%D8%A7%D8%B3%D8%AA%D8%B1%D8%A7%D9%84%DB%8C%D8%A7+%DA%A9%D8%AF%D8%A7%D9%85+%D8%A7%D8%B3%D8%AA%D8%9F+%3Csep%3E+%D9%85%D9%84%D8%A8%D9%88%D8%B1%D9%86+%3Csep%3E+%D8%B3%DB%8C%D8%AF%D9%86%DB%8C+%3Csep%3E+%DA%A9%D9%86%D8%A8%D8%B1%D8%A7+&fullscreen=true)  

## Reading Comprehension
- [mT5 trained on ParsiNLU-RC](https://huggingface.co/persiannlp/mt5-base-parsinlu-squad-reading-comprehension?text=%D9%82%D8%A7%D8%B1%D9%87+%D8%A2%D9%85%D8%B1%DB%8C%DA%A9%D8%A7+%D8%AF%D8%B1+%DA%86%D9%87+%D8%B3%D8%A7%D9%84%DB%8C+%DA%A9%D8%B4%D9%81+%D8%B4%D8%AF%D8%9F+%5Cn+%DB%8C%D8%B4+%D8%A7%D8%B2+%D8%AF%D9%87+%D9%87%D8%B2%D8%A7%D8%B1+%D8%B3%D8%A7%D9%84+%D8%A7%D8%B3%D8%AA+%DA%A9%D9%87+%D8%A7%D9%86%D8%B3%D8%A7%D9%86%E2%80%8C%D9%87%D8%A7+%D8%AF%D8%B1+%D9%82%D8%A7%D8%B1%D9%87%D9%94+%D8%A2%D9%85%D8%B1%DB%8C%DA%A9%D8%A7+%D8%B2%D9%86%D8%AF%DA%AF%DB%8C+%D9%85%DB%8C%E2%80%8C%DA%A9%D9%86%D9%86%D8%AF.+%D9%82%D8%A7%D8%B1%D9%87+%D8%A2%D9%85%D8%B1%DB%8C%DA%A9%D8%A7+%D8%AA%D9%88%D8%B3%D8%B7+%DA%A9%D8%B1%DB%8C%D8%B3%D8%AA%D9%81+%DA%A9%D9%84%D9%85%D8%A8+%D9%88+%D8%AF%D8%B1+%D8%B3%D8%A7%D9%84+%DB%B1%DB%B4%DB%B9%DB%B2+%DA%A9%D8%B4%D9%81+%D8%B4%D8%AF+%D8%A7%D9%85%D8%A7+%D8%A7%D9%88+%D8%A8%D9%87+%D8%A7%D8%B4%D8%AA%D8%A8%D8%A7%D9%87+%D9%81%DA%A9%D8%B1+%DA%A9%D8%B1%D8%AF+%DA%A9%D9%87+%D8%A2%D9%86%D8%AC%D8%A7+%D9%87%D9%86%D8%AF%D9%88%D8%B3%D8%AA%D8%A7%D9%86+%D8%A7%D8%B3%D8%AA+%D8%A7%D9%85%D8%A7+%D9%85%D8%AF%D8%AA%E2%80%8C%D9%87%D8%A7+%D8%A8%D8%B9%D8%AF+%D8%A2%D9%85%D8%B1%DB%8C%DA%AF%D9%88+%D9%88%D8%B3%D9%BE%D9%88%DA%86%DB%8C+%D8%A7%D8%B9%D9%84%D8%A7%D9%85+%DA%A9%D8%B1%D8%AF+%DA%A9%D9%87+%D8%A7%DB%8C%D9%86+%D9%82%D8%A7%D8%B1%D9%87+%D8%AC%D8%AF%DB%8C%D8%AF%DB%8C+%D8%A7%D8%B3%D8%AA.+%D8%A7%D9%85%D8%A7+%D8%AA%D8%A7%D8%B1%DB%8C%D8%AE+%D8%A2%D9%85%D8%B1%DB%8C%DA%A9%D8%A7+%D8%A8%D9%87+%D8%B9%D9%86%D9%88%D8%A7%D9%86+%DB%8C%DA%A9+%DA%A9%D8%B4%D9%88%D8%B1+%D9%85%D8%B3%D8%AA%D9%82%D9%84+%D8%A8%D9%87+%D8%B3%D8%A7%D9%84+%DB%B1%DB%B7%DB%B8%DB%B3+%D9%85%DB%8C%D9%84%D8%A7%D8%AF%DB%8C+%D8%A8%D8%A7%D8%B2%D9%85%DB%8C%E2%80%8C%DA%AF%D8%B1%D8%AF%D8%AF+%DA%A9%D9%87+%D8%AF%D8%B1+%D8%A2%D9%86+%D8%A2%D9%85%D8%B1%DB%8C%DA%A9%D8%A7+%D8%A8%D8%B1+%D8%B7%D8%A8%D9%82+%D9%85%D8%B9%D8%A7%D9%87%D8%AF%D9%87%D9%94+%D9%BE%D8%A7%D8%B1%DB%8C%D8%B3+%D8%A8%D9%87+%D8%B1%D8%B3%D9%85%DB%8C%D8%AA+%D8%B4%D9%86%D8%A7%D8%AE%D8%AA%D9%87+%DA%AF%D8%B1%D8%AF%DB%8C%D8%AF.&fullscreen=true) -  

## Translation
- [mT5 trained on ParsiNLU-MT](https://huggingface.co/persiannlp/mt5-base-parsinlu-opus-translation_fa_en) 

## Textual Entailment
- [mT5 trained on ParsiNLU-TE](https://huggingface.co/persiannlp/mt5-base-parsinlu-snli-entailment?text=%D8%A2%DB%8C%D8%A7+%DA%A9%D9%88%D8%AF%DA%A9%D8%A7%D9%86%DB%8C+%D9%88%D8%AC%D9%88%D8%AF+%D8%AF%D8%A7%D8%B1%D9%86%D8%AF+%DA%A9%D9%87+%D9%86%DB%8C%D8%A7%D8%B2+%D8%A8%D9%87+%D8%B3%D8%B1%DA%AF%D8%B1%D9%85%DB%8C+%D8%AF%D8%A7%D8%B1%D9%86%D8%AF%D8%9F+%3Csep%3E+%D9%87%DB%8C%DA%86+%DA%A9%D9%88%D8%AF%DA%A9%DB%8C+%D9%87%D8%B1%DA%AF%D8%B2+%D9%86%D9%85%DB%8C+%D8%AE%D9%88%D8%A7%D9%87%D8%AF+%D8%B3%D8%B1%DA%AF%D8%B1%D9%85+%D8%B4%D9%88%D8%AF.&fullscreen=true)   

## Query Paraphrasing
- [mT5 trained on ParsiNLU-QP](https://huggingface.co/persiannlp/mt5-base-parsinlu-qqp-query-paraphrasing?text=%D8%A2%DB%8C%D8%A7+%D9%84%DB%8C%D8%B2%D8%B1+%D9%85%D9%88%D9%87%D8%A7%DB%8C+%D8%B2%D8%A7%D8%A6%D8%AF+%D8%AF%D8%A7%D8%A6%D9%85%DB%8C+%D8%A7%D8%B3%D8%AA%D8%9F+%3Csep%3E+%D8%A2%DB%8C%D8%A7+%D9%84%DB%8C%D8%B2%D8%B1+%D9%85%D9%88%D9%87%D8%A7%DB%8C+%D8%B2%D8%A7%D8%A6%D8%AF+%D8%A8%D8%A7%D8%B9%D8%AB+%D9%81%D8%B1%D8%A7%D8%B1+%D8%AF%D8%A7%D8%A6%D9%85%DB%8C+%D8%A7%D8%B2+%D9%85%D9%88%D9%87%D8%A7%DB%8C+%D9%86%D8%A7%D8%AE%D9%88%D8%A7%D8%B3%D8%AA%D9%87+%D9%85%DB%8C+%D8%B4%D9%88%D8%AF%D8%9F&fullscreen=true)   

## Embeddings
- [Farsi Poem word2vec model](https://github.com/amnghd/Word2vec-on-Farsi-Literature) - This is a word2vec model deveoped based on a corpus of 48 Persian poets. The corpus consists of 1,216,286 mesras of Farsi poems and 8,102,119 words from which 148,588 are unique.
- [Sentence Transformers](https://github.com/m3hrdadfi/sentence-transformers) - ST is a collection of vector representations for sentences and paragraphs (also known as sentence embeddings). ST models are based on transformer networks like ParsBERT, ALBERT (soon). They are tuned based on Textual Thematic Similarity datasets such that sentences with similar meanings are close in vector space.

## Language Model
- [ParsBERT: Transformer-based Model for Persian Language Understanding)](https://github.com/hooshvare/parsbert) - It is a monolingual language model based on Googleâ€™s BERT architecture for the Persian Language only! This model is pre-trained on a large Persian corpus with various writing styles from numerous subjects (e.g., scientific, novels, news) with more than 2M documents. A large subset of this corpus was crawled manually.
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations for the Persian Language](https://github.com/m3hrdadfi/albert-persian) - ALBERT is the first attempt on ALBERT for the Persian Language. The model was trained based on Google's ALBERT BASE Version 2.0 over various writing styles from numerous subjects (e.g., scientific, novels, news) with more than 3.9M documents, 73M sentences, and 1.3B words, like the way we did for ParsBERT.
